{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":46283,"status":"ok","timestamp":1682536547358,"user":{"displayName":"Ayush Koirala","userId":"13726304305602096393"},"user_tz":-420},"id":"J8-V15NB-jOY","outputId":"4621ff88-1bd7-4f8c-f3ab-d625229e4a0f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting bert-extractive-summarizer\n","  Downloading bert_extractive_summarizer-0.10.1-py3-none-any.whl (25 kB)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.9/dist-packages (from bert-extractive-summarizer) (3.5.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.9/dist-packages (from bert-extractive-summarizer) (1.2.2)\n","Collecting transformers\n","  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m25.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.2.0)\n","Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.22.4)\n","Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.10.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (67.7.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (3.3.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (2.0.7)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (3.0.8)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (2.4.6)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (8.1.9)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (2.0.8)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (3.0.12)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (1.0.9)\n","Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (0.7.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (1.10.7)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (23.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (4.65.0)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (6.3.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (1.0.4)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (1.1.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (3.1.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (2.27.1)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy->bert-extractive-summarizer) (0.10.1)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers->bert-extractive-summarizer) (2022.10.31)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers->bert-extractive-summarizer) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers->bert-extractive-summarizer) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->bert-extractive-summarizer) (4.5.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers->bert-extractive-summarizer) (2023.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.26.15)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2022.12.7)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->bert-extractive-summarizer) (0.0.4)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy->bert-extractive-summarizer) (0.7.9)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy->bert-extractive-summarizer) (8.1.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy->bert-extractive-summarizer) (2.1.2)\n","Installing collected packages: tokenizers, huggingface-hub, transformers, bert-extractive-summarizer\n","Successfully installed bert-extractive-summarizer-0.10.1 huggingface-hub-0.14.1 tokenizers-0.13.3 transformers-4.28.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.28.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.11.0-py3-none-any.whl (468 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.7/468.7 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.4.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.14.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.1)\n","Collecting dill<0.3.7,>=0.3.0\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n","Collecting aiohttp\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (23.1.0)\n","Collecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.9.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (269 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.4/269.4 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.11.0 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.9.2\n"]}],"source":["!pip3 install bert-extractive-summarizer\n","!pip install transformers\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pqBKNIbJ_V66"},"outputs":[],"source":["import locale\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4TxD9fyK_eL0"},"outputs":[],"source":["!pip install sentencepiece"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5DjtotUU_fgJ"},"outputs":[],"source":["import torch\n","import datasets\n","import numpy as np\n","import pandas as pd\n","from datasets import load_dataset\n","from datasets import DatasetDict\n","#from transformers import GPT2LMHeadModel, GPT2TokenizerFast, AutoConfig\n","#from transformers import MT5ForConditionalGeneration, T5Tokenizer,AutoConfig\n","import torch\n","import torch.nn as nn\n","from transformers.optimization import Adafactor\n","import transformers\n","import torch\n","from summarizer import Summarizer"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Loading Xsum dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0re8lsIE_gvS"},"outputs":[],"source":["raw_datasets = load_dataset(\"xsum\")\n","train_data = raw_datasets['train']\n","test_data = raw_datasets['test']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cqskFKuj_iCi"},"outputs":[],"source":["summarizer = Summarizer()\n","\n","def extract_important_sentence(example, model):\n","  text = example['document']\n","  result = model(text, num_sentences=1)\n","  return {'important_sentences':result}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9iqt_Zp_kBy"},"outputs":[],"source":["raw_datasets = DatasetDict(\n","    {\n","        \"train\": train_data.select(range(500)),\n","        \"test\": test_data.select(range(50)),\n","    }\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e-3yQjzm_m3v"},"outputs":[],"source":["raw_datasets =  raw_datasets.map(lambda example:extract_important_sentence(example, summarizer))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lln3NpaF_m7p"},"outputs":[],"source":["raw_datasets"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Loading T5-small model "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLWdZVue_m-9"},"outputs":[],"source":["from transformers import T5Tokenizer, T5ForConditionalGeneration\n","tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n","model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYAg3RbK_nCC"},"outputs":[],"source":["tokenizer.add_special_tokens({'pad_token':'[PAD]'})"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Obtaining keywords from source document"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qyvdZTlH_nEg"},"outputs":[],"source":["import spacy\n","\n","# Load the spaCy model\n","nlp = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUNbzSxI_nHM"},"outputs":[],"source":["import spacy\n","\n","# Load the spaCy model\n","\n","nlp = spacy.load(\"en_core_web_sm\")\n","# Define a function to extract the top 5 keywords from a text\n","# Define a function to extract the top 5 keywords from a text\n","def extract_keywords(text,nlp):\n","    doc = nlp(text)\n","    # Get the noun chunks and their frequencies\n","    noun_chunks = {}\n","    for chunk in doc.noun_chunks:\n","        chunk_text = \" \".join([token.text for token in chunk if token.text.lower() != \"the\"])\n","        if chunk_text.lower() not in nlp.Defaults.stop_words:\n","            if chunk_text not in noun_chunks:\n","                noun_chunks[chunk_text] = 1\n","            else:\n","                noun_chunks[chunk_text] += 1\n","    # Sort the noun chunks by frequency and return the top 5\n","    sorted_chunks = sorted(noun_chunks.items(), key=lambda x: x[1], reverse=True)\n","    return ', '.join([chunk[0] for chunk in sorted_chunks[:20]])\n","\n","def keywords(example,nlp):\n","  # Loop through each sample in the train data and extract the top 5 keywords\n","    text = example[\"document\"]\n","    keywords = extract_keywords(text,nlp)\n","  \n","    return {\"Keywords\":keywords}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0NReEYB_nI3"},"outputs":[],"source":["nlp = spacy.load(\"en_core_web_sm\")\n","raw_datasets =  raw_datasets.map(lambda example:keywords(example,nlp))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0GfdT0NU_nMy"},"outputs":[],"source":["tokenizer.add_special_tokens({'pad_token':'[PAD]'})\n","PAD_IDX = tokenizer.pad_token_id\n","PAD_IDX"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Preprocessing xsum dataset and obtaining final input_ids,attention_mask and label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qQOABbsBGmy"},"outputs":[],"source":["prefix_len = 100\n","def preprocess_function(examples, prefix_len=100, padding=True):\n","    text_column = 'document'\n","    summary_column = 'summary'\n","    important_column = 'important_sentences'\n","    keyword_column = 'Keywords'\n","    max_source_length = 512  \n","    max_target_length = 512\n","\n","    text = examples[text_column]\n","    targets = examples[summary_column]\n","    important = examples[important_column]\n","    keyword = examples[keyword_column]\n","\n","    #inputs = \" \".join(text.split(' '))[:300]  #for each sample we are only taking 200 words\n","    #inputs = '<|endoftext|>' + inputs + 'TL;DR:' + targets + '<|endoftext|>'\n","    #tokenize inputs, label and important sentences\n","    #model_inputs = tokenizer(inputs, return_tensors=\"pt\", padding=padding, max_length=max_source_length, truncation=True)\n","    labels = tokenizer(targets, padding=padding, max_length=max_target_length, return_tensors=\"pt\", truncation=True)\n","    importance_sentences = tokenizer(important, padding=\"max_length\", max_length=prefix_len, return_tensors=\"pt\", truncation=True)\n","    importance_keywords = tokenizer(keyword, padding=\"max_length\", max_length=prefix_len, return_tensors=\"pt\", truncation=True)\n","    input_length = importance_sentences['input_ids'].shape[1]\n","    target_length = labels[\"input_ids\"].shape[1]\n","\n","    #print(max_source_length,target_length,prefix_len,input_length)\n","    #concat input ids with importance sentences in the front and label in the back\n","    importance_sentences['input_ids'] = torch.cat([importance_keywords['input_ids'],importance_sentences['input_ids'],torch.full((1,max_source_length-prefix_len-input_length), PAD_IDX)],1)\n","    #print('Input shape',model_inputs['input_ids'].shape)\n","    #concat label with -100 in the front and padding\n","    labels[\"input_ids\"] = torch.cat([torch.full((1,prefix_len),-100), labels['input_ids'],torch.full((1,max_target_length-prefix_len-target_length), PAD_IDX)],1)\n","    importance_sentences[\"labels\"] = labels[\"input_ids\"]\n","    #print(labels['input_ids'].shape)\n","    #concat attention with 0 in the back\n","    attention_length = importance_sentences[\"attention_mask\"].shape[1]\n","    importance_sentences[\"attention_mask\"] = torch.cat([importance_keywords['attention_mask'],importance_sentences['attention_mask'],torch.full((1,max_source_length-prefix_len-attention_length), 1)],1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DlLyOz2Y_nP7"},"outputs":[],"source":["column_names = ['document', 'summary','id','important_sentences','keywords']\n","new_train_dataset = raw_datasets[\"train\"].map(lambda example : preprocess_function(example,prefix_len),remove_columns=column_names)\n","new_test_dataset = raw_datasets[\"test\"].map(lambda example : preprocess_function(example,prefix_len),remove_columns=column_names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mle7rAjl_nSP"},"outputs":[],"source":["lambda example:extract_important_sentence(example, summarizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9YCsCBkoAfK6"},"outputs":[],"source":["from torch.utils.data import DataLoader\n","from torch.nn.utils.rnn import pad_sequence"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Making batch of xsum dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xp38Vz_tAfND"},"outputs":[],"source":["BATCH_SIZE = 8\n","new_train_dataset.set_format(\"torch\")\n","loader = DataLoader(new_train_dataset, batch_size=BATCH_SIZE)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QMZC2t6fAfS_"},"outputs":[],"source":["for batch in loader:\n","  break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bKBQL_09AfU4"},"outputs":[],"source":["class SoftEmbedding(nn.Module):\n","    def __init__(self, \n","                wte: nn.Embedding,\n","                n_tokens: int = 10, \n","                random_range: float = 0.5,\n","                initialize_from_vocab: bool = True):\n","        \"\"\"appends learned embedding to \n","        Args:\n","            wte (nn.Embedding): original transformer word embedding\n","            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n","            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n","            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n","        \"\"\"\n","        super(SoftEmbedding, self).__init__()\n","        self.wte = wte\n","        self.n_tokens = n_tokens\n","        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n","                                                                                  n_tokens, \n","                                                                                  random_range, \n","                                                                                  initialize_from_vocab))\n","            \n","    def initialize_embedding(self, \n","                             wte: nn.Embedding,\n","                             n_tokens: int = 10, \n","                             random_range: float = 0.5, \n","                             initialize_from_vocab: bool = True):\n","        \"\"\"initializes learned embedding\n","        Args:\n","            same as __init__\n","        Returns:\n","            torch.float: initialized using original schemes\n","        \"\"\"\n","        if initialize_from_vocab:\n","            return self.wte.weight[:n_tokens].clone().detach()\n","        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range)\n","            \n","    def forward(self, tokens):\n","        \"\"\"run forward pass\n","        Args:\n","            tokens (torch.long): input tokens before encoding\n","        Returns:\n","            torch.float: encoding of text concatenated with learned task specifc embedding\n","        \"\"\"\n","        input_embedding = self.wte(tokens[:, self.n_tokens:])\n","        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n","        return torch.cat([learned_embedding, input_embedding], 1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dpLR0CdAAfW4"},"outputs":[],"source":["model_setup_for_prompt_tuning=False\n","if not model_setup_for_prompt_tuning:\n","  #model.train()\n","\n","  # Freeze model\n","  for param in model.parameters():\n","      param.requires_grad = False\n","\n","  old_wte = model.get_input_embeddings()\n","\n","  # Add softembedding module\n","  s_wte = SoftEmbedding(old_wte,\n","                        n_tokens=prefix_len,\n","                        initialize_from_vocab=True).to(\"cuda\")\n","  model.set_input_embeddings(s_wte)\n","  if torch.cuda.is_available():\n","    model = model.cuda()\n","  # Set up optimizer\n","  parameters = list(model.parameters())\n","  # params = [model.transformer.wte.learned_embedding]\n","  # optimizer = Adafactor(params=params)\n","  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dFOHDcniAfZP"},"outputs":[],"source":["optimizer = torch.optim.Adam(s_wte.parameters(), lr=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dACO4q4NAfaw"},"outputs":[],"source":["import pickle\n","\n","iterations = 100\n","name = \"100_1_prefix_with_top_3_imp_sentences_t5\"\n","losses = []\n","best_loss = float('inf')\n","for batch in loader:\n","  for i in range(iterations):\n","      optimizer.zero_grad()\n","      input=batch[\"input_ids\"].to(\"cuda\")\n","      input = input.squeeze(1)\n","      attention = batch[\"attention_mask\"].to(\"cuda\")\n","      attention = attention.squeeze(1)\n","      label = batch[\"labels\"].cuda()\n","      label = label.squeeze(1)\n","      output = model(input_ids=input,attention_mask = attention, labels=label)      \n","      loss = output.loss\n","      losses.append(loss)\n","      loss.backward()\n","      optimizer.step()\n","      if i%10 == 0:\n","        print(f\"{i}: Loss: {loss}\")\n","  if loss < best_loss:\n","    best_loss = loss\n","    #torch.save(model.transformer.wte.learned_embedding, f'{name}.pt')\n","    pickle.dump(model, open(f\"{name}.pkl\",\"wb\"))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dm9KGHJtAr-r"},"outputs":[],"source":["final_losses = [loss.item() for loss in losses]"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyN3A+3ldrrNQybLrvABrqqC","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
